{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# merge df_umsatz_warengruppe, df_kiel_events, df_kiwo, df_Schiffahrt, df_wetter, df_wochentage\n",
    "# df_umsatz_warengruppe is the main dataframe, add the others via left merge\n",
    "# all dataframes are in the folder _Variables_cleaned\n",
    "\n",
    "df_umsatz_warengruppe = pd.read_csv('../_Variables_cleaned/df_umsatz_warengruppe.csv')\n",
    "df_kiel_events = pd.read_csv('../_Variables_cleaned/df_kiel_events.csv')\n",
    "df_kiwo = pd.read_csv('../_Variables_cleaned/df_kiwo.csv')\n",
    "df_Schifffahrt = pd.read_csv('../_Variables_cleaned/df_Schifffahrt.csv')\n",
    "df_wetter = pd.read_csv('../_Variables_cleaned/df_wetter.csv')\n",
    "df_wochentage = pd.read_csv('../_Variables_cleaned/df_wochentage.csv')\n",
    "\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_kiel_events, on='Datum', how='left')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_kiwo, on='Datum', how='outer')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_Schifffahrt, on='Datum', how='left')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_wetter, on='Datum', how='outer')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_wochentage, on='Datum', how='left')\n",
    "\n",
    "# save dataframe as df_prepared in _data_prepared\n",
    "df_umsatz_warengruppe.to_csv('../_data_prepared/df_dataset_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset_complete: Remove data before 01.07.2013 and after 30.0.7.2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all rows before 01.07.2013 and after 30.07.2019\n",
    "import pandas as pd\n",
    "\n",
    "# read in df_dataset_complete\n",
    "df_umsatz_warengruppe = pd.read_csv('../_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# filter out all rows before 01.07.2013 and after 30.07.2019\n",
    "df_umsatz_warengruppe = df_umsatz_warengruppe[(df_umsatz_warengruppe['Datum'] >= '2013-07-01') & (df_umsatz_warengruppe['Datum'] <= '2019-07-30')]\n",
    "\n",
    "# save df\n",
    "df_umsatz_warengruppe.to_csv('../_data_prepared/df_dataset_complete.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## INACTIVE CODE FROM MERGE-V2 ##\n",
    "# Datensatz alles von 01.07.2013 lÃ¶schen\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# # Convert Datum to datetime\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# # Filter dataset to keep only dates from 01.07.2013 onwards\n",
    "# cutoff_date = pd.to_datetime('2013-07-01')\n",
    "# df = df[df['Datum'] >= cutoff_date]\n",
    "\n",
    "# # Verify the date range\n",
    "# print(\"Earliest date in dataset:\", df['Datum'].min())\n",
    "# print(\"Latest date in dataset:\", df['Datum'].max())\n",
    "# print(\"Total rows in dataset:\", len(df))\n",
    "\n",
    "# # Overwrite the original CSV file\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Median (Inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# # Convert Datum to datetime\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# # Create mask for data up to cutoff date\n",
    "# cutoff_date = pd.to_datetime('2018-07-31')\n",
    "# mask = df['Datum'] <= cutoff_date\n",
    "\n",
    "# # Identify numerical columns\n",
    "# numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# # Count missing values for numerical columns up to the cutoff date\n",
    "# missing_values = df.loc[mask, numerical_columns].isnull().sum()\n",
    "\n",
    "# # Calculate percentage of missing values\n",
    "# missing_percentage = (df.loc[mask, numerical_columns].isnull().sum() / len(df.loc[mask])) * 100\n",
    "\n",
    "# # Combine count and percentage, sort by count in descending order\n",
    "# missing_summary = pd.DataFrame({\n",
    "#     'Missing Count': missing_values,\n",
    "#     'Missing Percentage (%)': missing_percentage\n",
    "# })\n",
    "\n",
    "# # Filter to show only columns with missing values\n",
    "# missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# print(\"Missing Values Analysis for Numerical Columns up to 2018-07-31:\")\n",
    "# print(missing_summary)\n",
    "# print(\"Missing Values Analysis for Data up to 2018-07-31:\")\n",
    "# print(missing_summary)\n",
    "\n",
    "# # Set cutoff date\n",
    "# cutoff_date = pd.to_datetime('2018-07-31')\n",
    "\n",
    "# # Identify numerical columns automatically\n",
    "# numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# # Create mask for data up to cutoff date\n",
    "# imputation_mask = df['Datum'] <= cutoff_date\n",
    "\n",
    "# # Print initial missing values for numerical columns\n",
    "# print(\"Missing values before imputation (up to 31.07.2018):\")\n",
    "# print(df.loc[imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# # Perform median imputation only on numerical columns up to cutoff date\n",
    "# median_imputer = SimpleImputer(strategy='median')\n",
    "# df.loc[imputation_mask, numerical_columns] = median_imputer.fit_transform(\n",
    "#     df.loc[imputation_mask, numerical_columns]\n",
    "# )\n",
    "\n",
    "# # Verify imputation results\n",
    "# print(\"\\nMissing values after imputation (up to 31.07.2018):\")\n",
    "# print(df.loc[imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# print(\"\\nMissing values after cut-off date:\")\n",
    "# print(df.loc[~imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# # Optional: Save the imputed dataframe\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-Schnipsel: Feature Tag ohne Umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code-Schnipsel: Feature Tag ohne Umsatz\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from workalendar.europe import Germany\n",
    "# from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday\n",
    "\n",
    "# class GermanHolidayCalendar(AbstractHolidayCalendar):\n",
    "#     rules = [Holiday(name=holiday[1], year=holiday[0].year, month=holiday[0].month, day=holiday[0].day) for holiday in Germany().holidays()]\n",
    "\n",
    "# def create_holiday_feature(df):\n",
    "#     # Initialize holiday calendar\n",
    "#     cal = GermanHolidayCalendar()\n",
    "    \n",
    "#     # Get holidays for the entire date range in the dataset\n",
    "#     start_year = df['Datum'].dt.year.min()\n",
    "#     end_year = df['Datum'].dt.year.max()\n",
    "    \n",
    "#     holidays = cal.holidays(start=f'{start_year}-01-01', end=f'{end_year}-12-31').to_pydatetime()\n",
    "#     holidays = pd.DataFrame(holidays, columns=['date'])\n",
    "    \n",
    "#     # Create holiday feature\n",
    "#     df['holiday_zero_sales'] = df['Datum'].isin(holidays['date']).astype(int)\n",
    "    \n",
    "#     # Optional: Add more nuanced holiday impact\n",
    "#     df['holiday_type'] = np.where(\n",
    "#         (df['Datum'].dt.month == 12) & (df['Datum'].dt.day.isin([25, 26])), \n",
    "#         'Christmas', \n",
    "#         np.where(\n",
    "#             (df['Datum'].dt.month == 1) & (df['Datum'].dt.day == 1), \n",
    "#             'NewYear',\n",
    "#             np.where(\n",
    "#                 (df['Datum'].dt.month == 5) & (df['Datum'].dt.day == 1), \n",
    "#                 'LaborDay',\n",
    "#                 np.where(\n",
    "#                     df['Datum'].isin(holidays['date']), \n",
    "#                     'Other', \n",
    "#                     'No Holiday'\n",
    "#                 )\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Example usage\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "# df = create_holiday_feature(df)\n",
    "# df.head()\n",
    "\n",
    "# # Save to the same file (overwrite original dataset)\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)\n",
    "\n",
    "# # OR save to a new file to keep the original intact\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_with_holidays.csv', index=False)\n",
    "\n",
    "# updated_df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "# print(updated_df.head())\n",
    "\n",
    "# # Drop the 'holiday_type' column\n",
    "# df = df.drop(columns=['holiday_type'])\n",
    "\n",
    "# # Save the updated DataFrame back to the file\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)\n",
    "\n",
    "# # Optional: Verify that the column has been removed\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data as df (df_merged_umsatz_kiwo01_wetter_wochentage.csv) in 1_DatasetCharacteristics\n",
    "df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# Den Datensatz df teilen in Trainingsdatensatz and Validierungsdatensatz\n",
    "# df_training: alle Daten vom 01.07.2013 bis 31.07.2017\n",
    "# df_validation: alle Daten vom 01.08.2017 bis 31.07.2018\n",
    "# df_test: alle Daten vom 01.08.2018 bis 30.07.2019\n",
    "\n",
    "df_training = df[(df['Datum'] >= '2013-07-01') & (df['Datum'] < '2017-08-01')]\n",
    "df_validation = df[(df['Datum'] >= '2017-08-01') & (df['Datum'] < '2018-08-01')]\n",
    "df_test = df[(df['Datum'] >= '2018-08-01') & (df['Datum'] <= '2019-07-30')]\n",
    "\n",
    "\n",
    "# save df_training and df_test as csv files\n",
    "df_training.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_training.csv', index=False)\n",
    "df_validation.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_validation.csv', index=False)\n",
    "df_test.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Behandlung von NAs (listwise deletion) nach Betrachtung dieser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df_dataset_complete and include all columns that contain NAs in any column\n",
    "# save as df_dataset_complete_NA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_complete = pd.read_csv('../_data_prepared/df_dataset_complete.csv')\n",
    "df_training = pd.read_csv('../_data_prepared/df_training.csv')\n",
    "df_validation = pd.read_csv('../_data_prepared/df_validation.csv')\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "# df = df_complete\n",
    "# df = df_training\n",
    "# df = df_validation\n",
    "# df = df_test\n",
    "\n",
    "# Identify columns with missing values\n",
    "# Define a function to filter and save datasets with missing values\n",
    "def filter_and_save_na(df, df_name):\n",
    "    columns_with_na = df.columns[df.isnull().any()].tolist()\n",
    "    df_na = df[df[columns_with_na].isnull().any(axis=1)]\n",
    "    df_na.to_csv(f'../_data_prepared/{df_name}_NA.csv', index=False)\n",
    "    # print(f\"{df_name} - Columns with missing values: {columns_with_na}\")\n",
    "    # print(f\"{df_name} - Shape of filtered dataset: {df_na.shape}\")\n",
    "    # print(df_na.head())\n",
    "\n",
    "# Apply the function to each dataframe\n",
    "filter_and_save_na(df_complete, 'df_complete')\n",
    "filter_and_save_na(df_training, 'df_training')\n",
    "filter_and_save_na(df_validation, 'df_validation')\n",
    "filter_and_save_na(df_test, 'df_test')\n",
    "\n",
    "# Load the datasets with missing values\n",
    "df_training_na = pd.read_csv('../_data_prepared/df_training_NA.csv')\n",
    "df_validation_na = pd.read_csv('../_data_prepared/df_validation_NA.csv')\n",
    "df_test_na = pd.read_csv('../_data_prepared/df_test_NA.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Behandlung der NAs je Spalte und je Datensatz\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# listwise deletion of missing values in df_training (df is already loaded)\n",
    "df_training = pd.read_csv('../_data_prepared/df_training.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_training_cleaned = df_training.dropna()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_training_cleaned.to_csv('../_data_prepared/df_training_cleaned.csv', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
