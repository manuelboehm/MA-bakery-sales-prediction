{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# merge df_umsatz_warengruppe, df_kiel_events, df_kiwo, df_Schiffahrt, df_wetter, df_wochentage\n",
    "# df_umsatz_warengruppe is the main dataframe, add the others via left merge\n",
    "# all dataframes are in the folder _Variables_cleaned\n",
    "\n",
    "df_umsatz_warengruppe = pd.read_csv('../_Variables_cleaned/df_umsatz_warengruppe.csv')\n",
    "df_kiel_events = pd.read_csv('../_Variables_cleaned/df_kiel_events.csv')\n",
    "df_kiwo = pd.read_csv('../_Variables_cleaned/df_kiwo.csv')\n",
    "df_Schifffahrt = pd.read_csv('../_Variables_cleaned/df_Schifffahrt.csv')\n",
    "df_wetter = pd.read_csv('../_Variables_cleaned/df_wetter.csv')\n",
    "df_wochentage = pd.read_csv('../_Variables_cleaned/df_wochentage.csv')\n",
    "\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_kiel_events, on='Datum', how='outer')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_kiwo, on='Datum', how='outer')\n",
    "# df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_Schifffahrt, on='Datum', how='left')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_wetter, on='Datum', how='outer')\n",
    "df_umsatz_warengruppe = pd.merge(df_umsatz_warengruppe, df_wochentage, on='Datum', how='outer')\n",
    "\n",
    "# save dataframe as df_prepared in _data_prepared\n",
    "df_umsatz_warengruppe.to_csv('../_data_prepared/df_dataset_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset_complete: Remove data before 01.07.2013 and after 30.07.2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all rows before 01.07.2013 and after 30.07.2019\n",
    "import pandas as pd\n",
    "\n",
    "# read in df_dataset_complete\n",
    "df_umsatz_warengruppe = pd.read_csv('../_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# filter out all rows before 01.07.2013 and after 30.07.2019\n",
    "df_umsatz_warengruppe = df_umsatz_warengruppe[(df_umsatz_warengruppe['Datum'] >= '2013-07-01') & (df_umsatz_warengruppe['Datum'] <= '2019-07-30')]\n",
    "\n",
    "# save df\n",
    "df_umsatz_warengruppe.to_csv('../_data_prepared/df_dataset_complete.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## INACTIVE CODE FROM MERGE-V2 ##\n",
    "# Datensatz alles von 01.07.2013 löschen\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# # Convert Datum to datetime\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# # Filter dataset to keep only dates from 01.07.2013 onwards\n",
    "# cutoff_date = pd.to_datetime('2013-07-01')\n",
    "# df = df[df['Datum'] >= cutoff_date]\n",
    "\n",
    "# # Verify the date range\n",
    "# print(\"Earliest date in dataset:\", df['Datum'].min())\n",
    "# print(\"Latest date in dataset:\", df['Datum'].max())\n",
    "# print(\"Total rows in dataset:\", len(df))\n",
    "\n",
    "# # Overwrite the original CSV file\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Median (Inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Load the dataframe\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# # Convert Datum to datetime\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# # Create mask for data up to cutoff date\n",
    "# cutoff_date = pd.to_datetime('2018-07-31')\n",
    "# mask = df['Datum'] <= cutoff_date\n",
    "\n",
    "# # Identify numerical columns\n",
    "# numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# # Count missing values for numerical columns up to the cutoff date\n",
    "# missing_values = df.loc[mask, numerical_columns].isnull().sum()\n",
    "\n",
    "# # Calculate percentage of missing values\n",
    "# missing_percentage = (df.loc[mask, numerical_columns].isnull().sum() / len(df.loc[mask])) * 100\n",
    "\n",
    "# # Combine count and percentage, sort by count in descending order\n",
    "# missing_summary = pd.DataFrame({\n",
    "#     'Missing Count': missing_values,\n",
    "#     'Missing Percentage (%)': missing_percentage\n",
    "# })\n",
    "\n",
    "# # Filter to show only columns with missing values\n",
    "# missing_summary = missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# print(\"Missing Values Analysis for Numerical Columns up to 2018-07-31:\")\n",
    "# print(missing_summary)\n",
    "# print(\"Missing Values Analysis for Data up to 2018-07-31:\")\n",
    "# print(missing_summary)\n",
    "\n",
    "# # Set cutoff date\n",
    "# cutoff_date = pd.to_datetime('2018-07-31')\n",
    "\n",
    "# # Identify numerical columns automatically\n",
    "# numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# # Create mask for data up to cutoff date\n",
    "# imputation_mask = df['Datum'] <= cutoff_date\n",
    "\n",
    "# # Print initial missing values for numerical columns\n",
    "# print(\"Missing values before imputation (up to 31.07.2018):\")\n",
    "# print(df.loc[imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# # Perform median imputation only on numerical columns up to cutoff date\n",
    "# median_imputer = SimpleImputer(strategy='median')\n",
    "# df.loc[imputation_mask, numerical_columns] = median_imputer.fit_transform(\n",
    "#     df.loc[imputation_mask, numerical_columns]\n",
    "# )\n",
    "\n",
    "# # Verify imputation results\n",
    "# print(\"\\nMissing values after imputation (up to 31.07.2018):\")\n",
    "# print(df.loc[imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# print(\"\\nMissing values after cut-off date:\")\n",
    "# print(df.loc[~imputation_mask, numerical_columns].isnull().sum())\n",
    "\n",
    "# # Optional: Save the imputed dataframe\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_imputed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-Schnipsel: Feature Tag ohne Umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code-Schnipsel: Feature Tag ohne Umsatz\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from workalendar.europe import Germany\n",
    "# from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday\n",
    "\n",
    "# class GermanHolidayCalendar(AbstractHolidayCalendar):\n",
    "#     rules = [Holiday(name=holiday[1], year=holiday[0].year, month=holiday[0].month, day=holiday[0].day) for holiday in Germany().holidays()]\n",
    "\n",
    "# def create_holiday_feature(df):\n",
    "#     # Initialize holiday calendar\n",
    "#     cal = GermanHolidayCalendar()\n",
    "    \n",
    "#     # Get holidays for the entire date range in the dataset\n",
    "#     start_year = df['Datum'].dt.year.min()\n",
    "#     end_year = df['Datum'].dt.year.max()\n",
    "    \n",
    "#     holidays = cal.holidays(start=f'{start_year}-01-01', end=f'{end_year}-12-31').to_pydatetime()\n",
    "#     holidays = pd.DataFrame(holidays, columns=['date'])\n",
    "    \n",
    "#     # Create holiday feature\n",
    "#     df['holiday_zero_sales'] = df['Datum'].isin(holidays['date']).astype(int)\n",
    "    \n",
    "#     # Optional: Add more nuanced holiday impact\n",
    "#     df['holiday_type'] = np.where(\n",
    "#         (df['Datum'].dt.month == 12) & (df['Datum'].dt.day.isin([25, 26])), \n",
    "#         'Christmas', \n",
    "#         np.where(\n",
    "#             (df['Datum'].dt.month == 1) & (df['Datum'].dt.day == 1), \n",
    "#             'NewYear',\n",
    "#             np.where(\n",
    "#                 (df['Datum'].dt.month == 5) & (df['Datum'].dt.day == 1), \n",
    "#                 'LaborDay',\n",
    "#                 np.where(\n",
    "#                     df['Datum'].isin(holidays['date']), \n",
    "#                     'Other', \n",
    "#                     'No Holiday'\n",
    "#                 )\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Example usage\n",
    "# df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "# df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "# df = create_holiday_feature(df)\n",
    "# df.head()\n",
    "\n",
    "# # Save to the same file (overwrite original dataset)\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)\n",
    "\n",
    "# # OR save to a new file to keep the original intact\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_with_holidays.csv', index=False)\n",
    "\n",
    "# updated_df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "# print(updated_df.head())\n",
    "\n",
    "# # Drop the 'holiday_type' column\n",
    "# df = df.drop(columns=['holiday_type'])\n",
    "\n",
    "# # Save the updated DataFrame back to the file\n",
    "# df.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv', index=False)\n",
    "\n",
    "# # Optional: Verify that the column has been removed\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behandlung von Duplicates in all Dataframes\n",
    "### Find all duplicates in df_dataset_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all exact duplicates rows in df_dataset_complete\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "\n",
    "# remove duplicates, keep only the first occurence, remove all rows that are completely empty\n",
    "df_dropDuplicates = df.drop_duplicates()\n",
    "\n",
    "# save df\n",
    "df_dropDuplicates.to_csv('../_data_prepared/df_dataset_complete.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data as df (df_merged_umsatz_kiwo01_wetter_wochentage.csv) in 1_DatasetCharacteristics\n",
    "df = pd.read_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_dataset_complete.csv')\n",
    "\n",
    "# Den Datensatz df teilen in Trainingsdatensatz and Validierungsdatensatz\n",
    "# df_training: alle Daten vom 01.07.2013 bis 31.07.2017\n",
    "# df_validation: alle Daten vom 01.08.2017 bis 31.07.2018\n",
    "# df_test: alle Daten vom 01.08.2018 bis 30.07.2019\n",
    "\n",
    "df_training = df[(df['Datum'] >= '2013-07-01') & (df['Datum'] < '2017-08-01')]\n",
    "df_validation = df[(df['Datum'] >= '2017-08-01') & (df['Datum'] < '2018-08-01')]\n",
    "df_test = df[(df['Datum'] >= '2018-08-01') & (df['Datum'] <= '2019-07-30')]\n",
    "\n",
    "\n",
    "# save df_training and df_test as csv files\n",
    "df_training.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_training.csv', index=False)\n",
    "df_validation.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_validation.csv', index=False)\n",
    "df_test.to_csv('/workspaces/MA-bakery-sales-prediction/_data_prepared/df_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Behandlung von NAs (listwise deletion) \n",
    "### Prüfung der Dataframes auf Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df_dataset_complete and include all columns that contain NAs in any column\n",
    "# save as df_dataset_complete_NA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_complete = pd.read_csv('../_data_prepared/df_dataset_complete.csv')\n",
    "df_training = pd.read_csv('../_data_prepared/df_training.csv')\n",
    "df_validation = pd.read_csv('../_data_prepared/df_validation.csv')\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "\n",
    "# Identify columns with missing values\n",
    "# Define a function to filter and save datasets with missing values\n",
    "def filter_and_load_na(df, df_name):\n",
    "    columns_with_na = df.columns[df.isnull().any()].tolist()\n",
    "    df_na = df[df[columns_with_na].isnull().any(axis=1)]\n",
    "    return df_na\n",
    "\n",
    "# Apply the function to each dataframe and load the datasets with missing values\n",
    "df_complete_na = filter_and_load_na(df_complete, 'df_complete')\n",
    "df_training_na = filter_and_load_na(df_training, 'df_training')\n",
    "df_validation_na = filter_and_load_na(df_validation, 'df_validation')\n",
    "df_test_na = filter_and_load_na(df_test, 'df_test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listwise Deletion of Missing Values (CHECK AGAIN!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped in df_training due to missing values in Umsatz: 30\n",
      "Number of rows dropped in df_training due to missing values in Bewoelkung: 6\n",
      "Number of rows dropped in df_validation due to missing values in Umsatz: 8\n",
      "Number of rows dropped in df_validation due to missing values in Bewoelkung: 64\n"
     ]
    }
   ],
   "source": [
    "# Behandlung der NAs je Spalte und je Datensatz\n",
    "# 1 df_training\n",
    "import pandas as pd\n",
    "\n",
    "# listwise deletion of missing values in df_training (df is already loaded)\n",
    "df_training = pd.read_csv('../_data_prepared/df_training.csv')\n",
    "\n",
    "# Drop rows with missing values in umsatz, state how many rows were dropped\n",
    "df_training_cleaned = df_training.dropna(subset=['Umsatz'])\n",
    "print(\"Number of rows dropped in df_training due to missing values in Umsatz:\", len(df_training) - len(df_training_cleaned))\n",
    "\n",
    "# drop rows with missing values in Bewoelkung and state how many rows were dropped\n",
    "df_training_cleaned_ = df_training_cleaned.dropna(subset=['Bewoelkung'])\n",
    "print(\"Number of rows dropped in df_training due to missing values in Bewoelkung:\", len(df_training_cleaned) - len(df_training_cleaned_))\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_training_cleaned_.to_csv('../_data_prepared/df_training.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 2 df_validation\n",
    "import pandas as pd\n",
    "\n",
    "# listwise deletion of missing values in df_validation (df is already loaded)\n",
    "df_validation = pd.read_csv('../_data_prepared/df_validation.csv')\n",
    "\n",
    "# Drop rows with missing values in umsatz and state how many rows were dropped\n",
    "df_validation_cleaned = df_validation.dropna(subset=['Umsatz'])\n",
    "print(\"Number of rows dropped in df_validation due to missing values in Umsatz:\", len(df_validation) - len(df_validation_cleaned))\n",
    "\n",
    "# drop rows with missing values in Bewoelkung and state how many rows were dropped\n",
    "df_validation_cleaned_ = df_validation_cleaned.dropna(subset=['Bewoelkung'])\n",
    "print(\"Number of rows dropped in df_validation due to missing values in Bewoelkung:\", len(df_validation_cleaned) - len(df_validation_cleaned_))\n",
    "\n",
    "# save the cleaned dataset\n",
    "df_validation_cleaned_.to_csv('../_data_prepared/df_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare df_test for prediction\n",
    "### Adding Wochentag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dummy value to existing columns Wochentag_Mo, Wochentag_Di, Wochentag_Mi, Wochentag_Do, Wochentag_Fr, Wochentag_Sa, Wochentag_So to df_test depending on the weekday of the date in column Datum (JJJ-MM-TT HH:MM:SS)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "# Convert Datum to datetime\n",
    "df_test['Datum'] = pd.to_datetime(df_test['Datum'])\n",
    "\n",
    "# Create dummy columns for weekdays\n",
    "df_test['Wochentag_Mo'] = (df_test['Datum'].dt.dayofweek == 0).astype(int)\n",
    "df_test['Wochentag_Di'] = (df_test['Datum'].dt.dayofweek == 1).astype(int)\n",
    "df_test['Wochentag_Mi'] = (df_test['Datum'].dt.dayofweek == 2).astype(int)\n",
    "df_test['Wochentag_Do'] = (df_test['Datum'].dt.dayofweek == 3).astype(int)\n",
    "df_test['Wochentag_Fr'] = (df_test['Datum'].dt.dayofweek == 4).astype(int)\n",
    "df_test['Wochentag_Sa'] = (df_test['Datum'].dt.dayofweek == 5).astype(int)\n",
    "df_test['Wochentag_So'] = (df_test['Datum'].dt.dayofweek == 6).astype(int)\n",
    "\n",
    "# Save the updated dataset\n",
    "df_test.to_csv('../_data_prepared/df_test.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all value of Warengruppe to the df (6 rows for each date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate rows in df_test so that every day is represented 6 times, with value of 1 to 6 in column Warengruppe for each value of Datum\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "# Create a new DataFrame with 6 duplicates for each row\n",
    "df_test_expanded = pd.concat([df_test] * 6, ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by Datum and Warengruppe\n",
    "df_test_expanded = df_test_expanded.sort_values(['Datum', 'Warengruppe'])\n",
    "\n",
    "# Add a new column 'Warengruppe' with values from 1 to 6\n",
    "df_test_expanded['Warengruppe'] = df_test_expanded.groupby('Datum').cumcount() + 1\n",
    "\n",
    "# add dummy encoding from Warengruppe to df_test_expanded (1 = Brot, 2 = Brötchen, 3 = Croissant, 4 = Konditorei, 5 = Kuchen, 6 = Saisonbrot), the columns are already named correctly\n",
    "df_test_expanded['Brot'] = (df_test_expanded['Warengruppe'] == 1).astype(int)\n",
    "df_test_expanded['Broetchen'] = (df_test_expanded['Warengruppe'] == 2).astype(int)\n",
    "df_test_expanded['Croissant'] = (df_test_expanded['Warengruppe'] == 3).astype(int)\n",
    "df_test_expanded['Konditorei'] = (df_test_expanded['Warengruppe'] == 4).astype(int)\n",
    "df_test_expanded['Kuchen'] = (df_test_expanded['Warengruppe'] == 5).astype(int)\n",
    "df_test_expanded['Saisonbrot'] = (df_test_expanded['Warengruppe'] == 6).astype(int)\n",
    "\n",
    "\n",
    "# Save the updated dataset\n",
    "df_test_expanded.to_csv('../_data_prepared/df_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the column id as a combination of date and Warengruppe (as needed for kaggle upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert column id df_test as first column from Datum and Warengruppe, format: \"YYMMDDWarengruppe\", Warengruppe in df_test is a number from 1 to 6 (the dataframe is already loaded)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "# Convert Datum to datetime\n",
    "df_test['Datum'] = pd.to_datetime(df_test['Datum'])\n",
    "\n",
    "# Create the ID column\n",
    "df_test['id'] = df_test['Datum'].dt.strftime('%y%m%d') + df_test['Warengruppe'].astype(str)\n",
    "\n",
    "# Reorder the columns\n",
    "df_test = df_test[['id'] + [col for col in df_test.columns if col != 'id']]\n",
    "\n",
    "# Save the updated dataset\n",
    "df_test.to_csv('../_data_prepared/df_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge created df with sample submission (df template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with sample submission \n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df = pd.read_csv('../2_BaselineModel/sample_submission.csv')\n",
    "\n",
    "# remove Umsatz from df\n",
    "df = df.drop(columns=['Umsatz'])\n",
    "\n",
    "df_test = pd.read_csv('../_data_prepared/df_test.csv')\n",
    "\n",
    "# merge sample_submission with df_test based on column id\n",
    "df_test = pd.merge(df, df_test, on='id', how='left')\n",
    "\n",
    "# Save the updated dataset\n",
    "df_test.to_csv('../_data_prepared/df_test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
